# -*- coding: utf-8 -*-
"""(P) Question_and_Answer_Generator_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NDlKQzV8OqBNsIcYTNBd4Q6_QVdBnULP
"""

!pip install -q peft transformers trl
#!pip install -q accelerate
!pip install -q bitsandbytes
!pip install -i https://pypi.org/simple/bitsandbytes
!pip install -q torch

import locale
locale.getpreferredencoding = lambda: "UTF-8"

!pip install -q -U auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/
!pip install -q -U langchain

!pip install -q faiss-gpu
!pip install -q langchain_community
!pip install -q sentence-transformers

from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    pipeline,
    logging,
)
import torch

from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
from langchain import HuggingFacePipeline, PromptTemplate, LLMChain

compute_dtype = getattr(torch, "float16")

quant_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=False,
)

from google.colab import drive
drive.mount('/content/drive')

# Load model directly
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("/content/drive/MyDrive/gemma-2b-it-Question-generation-en-sft-qlora-Borobudur")

model = AutoModelForCausalLM.from_pretrained("/content/drive/MyDrive/gemma-2b-it-Question-generation-en-sft-qlora-Borobudur",
                                             quantization_config=quant_config,
                                             device_map={"": 0})

doc = """
Candi Borobudur dibangun oleh Dinasti Sailendra. Candi Borobudur merupakan peninggalan Buddha terbesar di dunia yang dibangun antara tahun 780-840 Masehi. Pada masa itu, Dinasti Sailendra adalah penguasa di wilayah tersebut. Candi Borobudur didirikan sebagai tempat pemujaan Buddha dan tempat ziarah. Candi ini dirancang untuk memberikan petunjuk kepada manusia agar menjauhi nafsu dunia dan mencari pencerahan serta kebijaksanaan menurut ajaran Buddha.

Candi Borobudur ditemukan oleh tentara Inggris pada tahun 1814 di bawah pimpinan Sir Thomas Stamford Raffles. Seluruh area candi berhasil dibersihkan pada tahun 1835, sehingga candi ini bisa dipelajari.

Borobudur dibangun dengan gaya arsitektur Mandala yang mencerminkan pandangan Buddha tentang alam semesta. Struktur candi ini berbentuk persegi dengan empat pintu masuk dan titik pusat berbentuk lingkaran. Jika dilihat dari luar ke dalam, candi ini terbagi menjadi dua bagian, yaitu bagian luar yang terdiri dari tiga zona yang melambangkan alam dunia dan bagian pusat yang melambangkan alam Nirwana."""

from transformers import pipeline
import re

def generate_question(context):

  pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

  messages = [{"role": "user", "content": (f"Berikut ini adalah konteks yang diberikan:\n\n{context}\n\n"
        "Berdasarkan konteks di atas, buatkan 5 pertanyaan singkat dan relevan. "
        "Pastikan setiap pertanyaan hanya berfokus pada informasi yang disajikan dalam konteks, menggunakan bahasa yang jelas dan mudah dipahami, dan tidak menanyakan informasi di luar konteks yang diberikan.\n"
        "Buatlah 5 pertanyaan berdasarkan konteks yang diberikan.")}]

  prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

  outputs = pipe(prompt, do_sample=True, temperature=0.8, top_k=50, top_p=0.95, max_new_tokens=1000)

  generated_question = outputs[0]["generated_text"][len(prompt):]

  # Define the regular expression pattern
  pattern = r"\d+\.\s+(.+?)\?"

  # Compile the regular expression pattern
  regex = re.compile(pattern)

  # Find all matches in the question list
  matches = regex.findall(generated_question)

  question_list = []
  # Print the extracted texts
  for match in matches:
      question_list.append(match.strip())

  # Menampilkan list pertanyaan
  return question_list

#question = generate_question(doc)
#question

"""## Generate Answer"""

# from transformers import AutoTokenizer, AutoModelForCausalLM

# model_id = "/content/drive/MyDrive/Llama3-ChatQA-1.5-8B-Borobudur"

# tokenizer = AutoTokenizer.from_pretrained(model_id)

# model = AutoModelForCausalLM.from_pretrained(
#         model_id,
#         device_map={"": 0},
#         quantization_config=quant_config)

# from transformers import AutoTokenizer, AutoModelForCausalLM

# model_id = "nvidia/Llama3-ChatQA-1.5-8B"

# tokenizer = AutoTokenizer.from_pretrained(model_id)

# model = AutoModelForCausalLM.from_pretrained(
#         model_id,
#         device_map={"": 0},
#         quantization_config=quant_config)

# import torch
# import transformers

# pipeline2 = transformers.pipeline(
#         "text-generation",
#         model=model_gptq,
#         tokenizer=tokenizer,
#         use_cache=True,
#         device_map="auto",
#         max_new_tokens=500,
#         temperature=0.1,
#         return_full_text=False,
#         do_sample=True,
#         top_k=10,
#         num_return_sequences=1,
#         eos_token_id=tokenizer.eos_token_id,
#         pad_token_id=tokenizer.eos_token_id, )

# import locale
# locale.getpreferredencoding = lambda: "UTF-8"

# from langchain import HuggingFacePipeline, PromptTemplate, LLMChain

# llm = HuggingFacePipeline(pipeline=pipeline2)

# from transformers import pipeline
# from langchain import HuggingFacePipeline

# terminators = [
#     tokenizer.eos_token_id,
#     tokenizer.convert_tokens_to_ids("<|eot_id|>")
# ]

# pipe = pipeline(
#     "text-generation",
#     model=model,
#     tokenizer=tokenizer,
#     max_new_tokens=512,
#     temperature=0.2,
#     return_full_text= False,
#     do_sample = True,
#     num_return_sequences=1,
#     top_k=10,
# )

# hf = HuggingFacePipeline(pipeline=pipe)

import torch
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig, LlamaTokenizer

model_id = "afrizalha/Kancil-V1-llama3-4bit"
config = AutoConfig.from_pretrained(model_id)

BNB_CONFIG = BitsAndBytesConfig(load_in_4bit=True,
                                bnb_4bit_use_double_quant=True,
                                bnb_4bit_quant_type="nf4",
                                bnb_4bit_compute_dtype=torch.bfloat16,
)

model = AutoModelForCausalLM.from_pretrained(model_id,
                                             quantization_config=BNB_CONFIG,
                                             device_map="auto",
                                             trust_remote_code=True)

tokenizer = AutoTokenizer.from_pretrained(model_id)

from transformers import pipeline
from langchain import HuggingFacePipeline

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=512,
    temperature=0.2,
    return_full_text= False,
    do_sample = True,
    num_return_sequences=1,
    top_k=10,
)

hf = HuggingFacePipeline(pipeline=pipe)

def answer_question(context, questions):
  template = """
            <|user|>
            Anda adalah asisten yang membantu menjawab pertanyaan dengan SANGAT SINGKAT dan JELAS. Gunakan HANYA informasi dari konteks yang diberikan, dan JANGAN mengulang kalimat pertanyaan. Berikan jawaban dengan MAKSIMAL 2 KALIMAT, menggunakan bahasa yang jelas dan mudah dipahami.
            Berikut adalah contoh pertanyaan dan jawaban yang baik:
            Pertanyaan: Siapa yang membangun Candi Borobudur?
            Jawaban: Dinasti Sailendra.
            {context}
            USER: {question}
            <|assistant|>
            """
  answer = []

  prompt_context = PromptTemplate(input_variables=["context", "question"], template=template)

  llm_chain_context = LLMChain(prompt=prompt_context ,llm=hf)

  for count, question in enumerate(questions):
    res =  llm_chain_context({"context": context , "question": question})['text'].strip()
    print(f"==== Pertanyaan {count+1} ====")
    print(f"Pertanyaan : {question}")
    print(f"Jawaban    : {res}\n\n")
    answer.append(res)

  return answer

"""Selecting Question"""

# questions_answers = list(zip(generated_question, answered_question))

def select_questions(questions_answers):
    selected = []
    for i, (question, answer) in enumerate(questions_answers, start=1):
        print(f"Pertanyaan {i}: {question}")
        include = input("Apakah Anda ingin memasukkan pertanyaan ini ke dalam list? (y/n): ")
        if include.lower() == 'y':
            selected.append((question, answer))
    return selected

# selected_questions = select_questions(questions_answers)

# print("\nPertanyaan dan jawaban terpilih:\n")
# for q, a in selected_questions:
#     print(f"Pertanyaan: {q}")
#     print(f"Jawaban: {a}")
#     print()

"""### Similarity Check"""

from langchain_community.document_loaders.csv_loader import CSVLoader
from langchain_community.document_loaders import DataFrameLoader
import pandas as pd

df = pd.read_csv("/content/drive/MyDrive/QnA_Result (2).csv")
df_question = df[["Pertanyaan"]]
df_question

loader = DataFrameLoader(df_question, page_content_column="Pertanyaan").load()

from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(model_name="firqaaa/indo-sentence-bert-base")
db = FAISS.from_documents(loader, embeddings)

def similarity_check(questions, answers):
  saved_question = []
  saved_answer = []
  threshold = 100

  print("\n\n======================")
  print(f"|   Threshold : {threshold}  |")
  print("======================\n")

  for i in range(len(questions)):
    docs_and_scores = db.similarity_search_with_score(questions[i])
    document, score = docs_and_scores[0]
    print(f"Pertanyaan ke - {i+1}")
    print(f"Pertanyaan : {questions[i]}")
    print(f"===================")

    if score > threshold:
      print(f"Similarity Score : {score}")
      print(f"Pertanyaan Disimpan....\n\n")
      saved_question.append(questions[i])
      saved_answer.append(answers[i])
    else:
      print(f"Similarity Score : {score}")
      print(f"Pertanyaan gagal Disimpan....\n\n")

  return saved_question, saved_answer

def qna_pipeline(context):
    # Generate questions and answers
    generated_question = generate_question(context)
    generated_question = [text.replace("**", "") for text in generated_question]
    answered_question = answer_question(context, generated_question)

    # Select questions
    questions_answers = list(zip(generated_question, answered_question))
    selected_questions_answers = select_questions(questions_answers)

    selected_questions, selected_answers = zip(*selected_questions_answers)

    # Similarity check
    similarity_question_check = similarity_check(selected_questions, selected_answers)
    return selected_questions, answered_question, similarity_question_check

#selected_questions, answered_question, similarity_question_check = qna_pipeline(doc)

similarity_question_check

"""#### Saved QnA"""

new_question = [i for i in similarity_question_check[0]]
new_answer = [i for i in similarity_question_check[1]]

print(new_question)
print(new_answer)

new_data = {
        "Pertanyaan": new_question,
        "Jawaban": new_answer
    }
new_df = pd.DataFrame(new_data)

df = pd.read_csv("/content/drive/MyDrive/QnA_Result (2).csv")

updated_df = pd.concat([df, new_df], ignore_index=True)

updated_df

updated_df.to_csv("/content/drive/MyDrive/QnA_Result (2).csv", index=False)

"""Gradio"""

!pip install -q gradio

import gradio as gr

# Gradio interface
def inference(contexts):
    generated_question = generate_question(contexts)
    answered_question = answer_question(contexts, generated_question)
    questions_answers = list(zip(generated_question, answered_question))
    return questions_answers

def save_selected(questions_answers, selected_texts):
    selected_questions = []
    selected_answers = []
    for text in selected_texts:
        for q, a in questions_answers:
            if f"Question: {q}, Answer: {a}" == text:
                selected_questions.append(q)
                selected_answers.append(a)
                break
    similarity_question_check = similarity_check(selected_questions, selected_answers)
    new_question = [i for i in similarity_question_check[0]]
    new_answer = [i for i in similarity_question_check[1]]
    new_data = {"Pertanyaan": new_question, "Jawaban": new_answer}
    new_df = pd.DataFrame(new_data)
    df = pd.read_csv("/content/drive/MyDrive/QnA_Result (2).csv")
    updated_df = pd.concat([df, new_df], ignore_index=True)
    updated_df.to_csv("/content/drive/MyDrive/QnA_Result (2).csv", index=False)
    print("New data added:")
    print(new_df)
    print("Updated dataset:")
    print(updated_df)

def display_questions(context):
    questions_answers = inference(context)
    choices = [f"Question: {q}, Answer: {a}" for q, a in questions_answers]
    return choices, questions_answers

def on_submit(context):
    choices, questions_answers = display_questions(context)
    return gr.update(choices=choices), questions_answers

with gr.Blocks(css=".title { font-size: 24px; font-weight: bold; color: #2E86C1; text-align: center; margin-bottom: 20px;}") as iface:
    gr.Markdown("<div class='title'>Borobudur QnA Generator</div>")
    gr.Markdown("Enter your context to generate questions and answers. Select the questions you wish to save for later use.")

    context_input = gr.Textbox(lines=7, label="Context", placeholder="Enter your context here", elem_id="context-input")

    submit_button = gr.Button("Generate Question and Answer", elem_id="submit-button")
    generated_questions = gr.CheckboxGroup(label="Generated Questions and Answers", elem_id="generated-questions")

    questions_answers_state = gr.State()
    submit_button.click(fn=on_submit, inputs=context_input, outputs=[generated_questions, questions_answers_state])

    save_button = gr.Button("Save Selected QnA", elem_id="save-button")
    #download_file = gr.File(label="Download Updated CSV file", elem_id="download-file")
    save_button.click(fn=save_selected, inputs=[questions_answers_state, generated_questions])

iface.launch(share=True, debug=True)

